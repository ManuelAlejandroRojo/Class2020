---
title: "General Linear Model"
author: "F. A. Barrios<br><small>Instituto de Neurobiolog√≠a UNAM<br></small>"
date: "<small>`r Sys.Date()`</small>"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: true
    toc_float:
      collapsed: false
    cod_folding: hide
    theme: cerulean
  pdf_document: default
description: "to prepare Class2020 presentations"
---

---
```{r setup, echo=FALSE}
require(Hmisc)
knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=TRUE)
# If using blogdown: knitrSet(lang='blogdown')
# knitr::opts_chunk$set(echo = TRUE)
```
`r hidingTOC(buttonLabel="Outline")`

```{r}
setwd("~/Dropbox/Fdo/ClaseStats/RegressionClass/RegressionR_code")
# To set the working directory at the user dir
library(tidyverse)
library(multcomp)
library(car)
library(emmeans)

hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")
```
# General Linear Model GLM (Modelo lineal General)
## Linear Regression
The term "regression" was introduced by Francis Galton (Darwin's nephew) during the XIX century to describe a biological phenomenon.  The heights of the descendants of tall ancestors have the tendency to "return", come back, to the normal average high in the population, known as the regression to the media. (Mr. Galton was an Eugenics supporter)

## Examples for "simple" linear regression
The general equation for the straight line is $y = mx + b_0$, this form is the "slope, intersection form". The slope is the rate of change the gives the change in $y$ for a unit change in $x$. Remember that the slope formula for two pair of points $(x_1, y_1)$ and $(x_2, y_2)$ is:
$$ m = \frac{(y_2 - y_1)}{(x_2 - x_1)}$$
We will run examples from the chapter 9 from Daniel's book. We sill solve simple (one variable) linear regressions, where the $\beta_0$ is the intercept value (this is the value taken by the model at $x_1=0$) and the adjusted $\beta_1$ is the slope of the model, the rate of change in $Y$ for a "unit of $x_1$ change"
$$Y=\beta_0 + \beta_1 x_1$$

```{r}
setwd("~/Dropbox/Fdo/ClaseStats/RegressionClass/RegressionR_code")
# Changing wd to load the data file
Exa9.3 = read.csv(file="DataOther/EXA_C09_S03_01.csv", header=TRUE)
names(Exa9.3)
plot(Exa9.3$Y ~ Exa9.3$X, pch = 20)
Ybar=mean(Exa9.3$Y)
Xbar=mean(Exa9.3$X)
abline(h=Ybar, col = 2, lty = 2)
abline(v=Xbar, col = 2, lty = 2)
Lin9.3 = lm(Y ~ X, data=Exa9.3)
summary(Lin9.3)
abline(Lin9.3, col=2)
```

Following with more examples from Daniel's book.
```{r}
setwd("~/Dropbox/Fdo/ClaseStats/RegressionClass/RegressionR_code")
# Changing wd to load the data file
# Problem 9.3.3 Methadone dose and the QTc Ventricular 
# Tachycardia
Exr3.3=read.csv(file="DataOther/EXR_C09_S03_03.csv", header=TRUE)
names(Exr3.3)
plot(Exr3.3$QTC ~ Exr3.3$DOSE, pch=20)
LinExr3.3 = lm(QTC ~ DOSE, data=Exr3.3)
summary(LinExr3.3)
abline(LinExr3.3, col=2)
# Res y = 559.9 + 0.139 x
```

## GLM
Response variable $Y$ is a random variable that is measured and has a distribution with expected value $E(Y|x)$ given a set of independent variables $x$.
$$Y_j (j=1, . . . , J)$$
for a set of $x_jl$ predictor variables (or independent variables) defined as vectors for each $j$
$$ x_jl (l=1, . . . , L)$$
with $L(L<J$, a general linear model with an error function $\epsilon_j$ can be expressed:
$$Y_j = x_{j1}\beta_1 + x_{j2}\beta_2 + x_{j3}\beta_3 + . . . + x_{jL}\beta_L + \epsilon_j$$
with $\epsilon_j$ an independent variable identically distributed to the Normal with mean equal to zero.
$$\epsilon_j \approx N(0,\sigma^2)_{iid}$$
## Linear Regresion (Chap 4, Vittinghoff et all.)

Example of simple linear regression: exercise and glucose Glucose levels above 125 mg/dL are diagnostic of diabetes, while 100-125 mg/dL signal increased risk.
Data from HERS (public data) has baseline of glucose levels among 2,032 participants in a clinical trial of Hormone Therapy (HT). Women with diabetes are excluded, to study if the exercise might help prevent progression to diabetes.

```{r}
# hers data structure
hers_nodi <- filter(hers, diabetes == "no")
hers_nodi_Fit <- lm(glucose ~ exercise, data = hers_nodi)
# the linear model results can be printed using summary
summary(hers_nodi_Fit)
```

Simple linear regression model shows coefficient estimate ($\beta_1$) for exercise shows that average baseline glucose levels were about 1.7mg/dL lower among women who exercised at least three times a week than among women who exercised less.

## For a multiple linear model
There are models to regress several predictor variables to relate several random independent variables.

$$y_i = E[y_i|x_i] + \epsilon_i$$
$$Y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \dots + \beta_p x_{p}$$
Multiple linear regression model coefficients, the betas, give the change in $E[Y|x]$ for an increase of one unit on the predictor $x_j$ , holding other factors in the model constant; each of the estimates is adjusted for the effects of all the other predictors. As in the simple linear model the intercept $\beta_0$ (beta zero) gives the value $E[Y|x]$ when all the predictors are equal to zero. Example of multiple linear model estimate is done with:  glucose ~ exercise + age + drinkany + BMI.

In general in R we can write:$Y = \beta_1 variable_1 + \beta_2 variable_2 + \beta_3 variable_3 + \beta_4 variable_4$ for a multiple linear model, in this case four regressors.

```{r}
hers_nodi_multFit <- lm(glucose ~ exercise + age + drinkany + BMI, data = hers_nodi)
# the linear model results can be printed using summary
summary(hers_nodi_multFit)
```
## Multiple linear model, with interactions
In general in R we can write the interaction term as the product of the regressors that we are studying the interaction:$variable_1:varible_2$ for a multiple linear model with two regressors and interaction the equation looks like:

$$Y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$
(The following is a very good link: http://www.sthda.com/english/articles/40-regression-analysis/ )
