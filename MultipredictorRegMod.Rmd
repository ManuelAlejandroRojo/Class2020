---
title: "MultipredictorRegMod.Rmd"
author: "F. A. Barrios<br><small>Instituto de Neurobiología, UNAM<br></small>"
date: "<small>`r Sys.Date()`</small>"
output:
  pdf_document: default
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: true
    toc_float:
      collapsed: false
    cod_folding: hide
    theme: cerulean
description: "example from chap 4, Vittinghoff"
---

```{r setup, echo=FALSE}
require(Hmisc)
knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=TRUE)
# If using blogdown: knitrSet(lang='blogdown')
# knitr::opts_chunk$set(echo = TRUE)
```

`r hidingTOC(buttonLabel="Outline")`

```{r}
# Multilevel categorical predictors Chap 4 Vittinghoff
# libraries
#
library(car)
library(emmeans)
library(multcomp)
library(tidyverse)
library(MASS)
#
```


# Exercise and Glucose (4.1 Vittinghoff)

Glucose levels above 125 mg/dL are diagnostic of diabetes, while levels in the range from 100 to 125 mg/dL signal increased risk of progressing to this serious and increasingly widespread condition. So it is of interest to determine whether exercise, a modifiable lifestyle factor, would help people reduce their glucose levels and thus avoid diabetes.
The R code shows a simple linear model using a measure of exercise to predict baseline glucose levels among 2,032 participants without diabetes, and boxplot of the data, in the HERS. Women with diabetes are excluded because the research question is whether exercise might help to prevent progression to diabetes among women at risk, and because the causal determinants of glucose may be different in that group.

```{r}
setwd("~/Dropbox/GitHub/Class2020")
# Chapter 4 examples. 4.1
hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")
hers_nodi <- filter(hers, diabetes == "no")
# Simple linear model with HERS data for women without diabetes
ggplot(data = hers_nodi, mapping = aes(x = exercise, y = glucose)) + 
  geom_boxplot(na.rm = TRUE) + facet_grid( . ~ diabetes) + geom_jitter(height = 0.15, width = 0.15)
# The simple linear model adjust the exercise like in table 4.1
hers_nodi_Fit <- lm(glucose ~ exercise, data = hers_nodi)
summary(hers_nodi_Fit)
```

The "coefficients" for the variable "exerciseyes" shows that average baseline glucose levels were about 1.7mg/dL lower among women who exercised at least three times a week than among women who exercised less. This difference is statistically significant ($t = -3.87, P < 0.000113$).
However, women who exercise are slightly younger, a little more likely to use alcohol, and in particular have lower average BMI, all factors associated with glucose levels. This implies that the lower average glucose we observe among women who exercise could be due at least in part to differences in these other predictors (independent variables). Under these conditions, it is important that our estimate of the difference in average glucose levels associated with exercise be “adjusted” for the effects of these potential confounders of the unadjusted association. Ideally, adjustment using a multipredictor regression model provides an estimate of the causal effect of exercise on average glucose levels, by holding the other variables constant.

# The multiple regression model (4.2)

In the multiple regression model the expected value $E[y \mid x]$ (expected value of the response function y given the vector x) is
$$E[y \mid x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots + \beta_p x_p$$
where $x$ represents the collection of $p$ predictors $x_1, x_2, \ldots x_p$ in the model, and the $\beta$ are the corresponding regression coefficients.

```{r}
# Chap 4 4.2 Multiple linear regresor model
# and to obtain the table 4.2 with multiple linear model

hers_nodi_Fit2 <- lm(glucose ~ exercise + age + drinkany + BMI, data = hers_nodi)
S(hers_nodi_Fit2)
#
```

in a multiple regression model that also includes —that is, adjusts for—age, alcohol use (drinkany), and BMI, average glucose is estimated to be only about 1 mg/dL lower among women who exercise, holding the other three factors constant. The multipredictor model also shows that average glucose levels are about 0.7 mg/dL higher among alcohol users than among nonusers. Average levels also increase by about 0.5 mg/dL per unit increase in BMI, and by 0.06 mg/dL for each additional year of age. Each of these associations is statistically significant after adjustment for the other predictors in the model.

## Interpretation of Adjusted Regression Coefficients

The coefficient $\beta_j, j=1, \ldots ,p$ gives the change in $E[y \mid x]$ for an increase of one unit in predictor $x_j$ , holding other factors in the model constant; each of the estimates is adjusted for the effects of all the other predictors. As in the simple linear model, the intercept $\beta_0$ gives the value of $E[y \mid x]$ when all the predictors are equal to zero.

## Generalization of R-squared and r

The coefficient of determination $R^2$ is the proportion of the total variability of the outcome that can be accounted for by the predictors. And the multiple correlatio coefficient $r = \sqrt{R^2}$  represents the correlation between the outcome $y$ and the fitted values $\hat{y}$.

# Categorical Predictors

Predictors in both simple and multiple predictor regression models can be binary, categorical, or discrete numeric, as well as continuous numeric.


## Binary Predictors

Binary predictors, a group with a characteristic and other group with out the characteristic, can be coded with a dummy variable, an indicator or dummy variable that can take value "1" for the group with the chracteristic and "0" for the group without the characteristic. With this coding, the regression coefficient corresponding to this variable has a straightforward interpretation as the increase or decrease in average outcome levels in the group with the characteristic, with respect to the reference group. With this coding for binary variables 1 = yes and 0 = no $\beta_0$ is the average of the baseline variable and $\beta_0 + \beta_1$ is related to the value for the "yes" condition ("yes" + average).

## Multilevel Categorical Predictors (4.3)

The 2,763 women in the HERS cohort also responded to a question about how physically active they considered themselves compared to other women their age. The five-level response variable "physact" ranged from “much less active” to “much more active,” and was coded in order from 1 to 5. This is an example of an ordinal variable. Multilevel categorical variables can also be nominal, in the sense that there is no intrinsic ordering in the categories. Examples include ethnicity, marital status, occupation, and geographic region. With nominal variables, it is even clearer that the numeric codes often used to represent the variable in the database cannot be treated like the values of a numeric variable.
Categorical variables are easily accommodated in multipredictor linear and other regression models, using indicator or dummy variables. As with binary variables, where two categories are represented in the model by a single indicator variable, categorical variables with $K \leq 2$ levels are represented by $K - 1$ indicators, one for each of level of the variable except a baseline or reference level.

```{r}
# Chap 4  4.3 Categorical predictors
# we are using the same file hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")
# Multilevel categorical predictors using the linear model for women without diebetes
# IMPORTANT compare with table 4.4 Regression of physical activity on glucose

hers_nodi <- mutate(hers_nodi, physact = factor(physact, levels=c("much less active","somewhat less active","about as active","somewhat more active","much more active")))
levels(hers_nodi$physact)
ggplot(data = hers_nodi, mapping = aes(x = physact, y = glucose)) + geom_boxplot(na.rm = TRUE)
glucose_fit_act <- lm(glucose ~ physact, data = hers_nodi)
#
Anova(glucose_fit_act, type="II")
S(glucose_fit_act)
layout(matrix(1:4, nrow = 2))
plot(glucose_fit_act)
# To compute the estimates marginal means for specified factors or factor combinations in a linear model
glucose_emmeans <- emmeans(glucose_fit_act, "physact")
summary(glucose_emmeans)
#
```

the corresponding $\beta_i$ have a straightforward interpretation. For the moment, consider a simple regression model in which the five levels of "physact"
are the only predictors:
$$E[glucose \mid x] = \beta_0 + \beta_2 SomewhatLessActive + \beta_3 AboutAsActive + \beta_4 SomewhatMoreActive + \beta_5 MuchMoreActive$$
Without other predictors, or covariates, the model is equivalent to a one-way ANOVA.
The parameters of the model can be manipulated to give the estimated mean in any group, or to give the estimated differences between any two groups. Estimated differences are the contrasts.

## Multiple Pairwise Comparisons Between Categories

It is frequently of interest to examine multiple pairwise differences between levels of a categorical predictor, especially when the overall F -test is statistically significant, and in some cases even when it is not.
For this case, various methods are available for controlling the familywise error rate (FER) for the wider set of comparisons being made. These methods differ in the trade-off made between power and the breadth of the circumstances under which the type-I error rate is protected. One of the most straightforward is Fisher’s least significant difference (LSD) procedure, in which the pairwise comparisons are carried out using t -tests at the nominal type-I error rate, but only if the overall F-test is statistically significant.
More conservative procedures that protect the FER (FWE) under partial null hypotheses include setting the level of the pairwise tests required to declare statistical significance
equal to $\alpha/k$ (Bonferroni) or$1-(1-\alpha)^{1/k}$ (Sidak), where $\alpha$ is the desired FER and $k$ is the number of preplanned comparisons to be made. The Sidak correction
is slightly more liberal for small values of $k$, but otherwise equivalent.

```{r}
# Contrasts
# contrasts using the adjusted parameters for a categorical variable with several categories
# or multiple ordinals
# R call categorical variables factors and their categories levels so we have factors with different
# levels in these cases we can estimate contrasts of the adjusted parameters.

Contrast_Table4.6 <- list(gluc_b0 = c(1, 0, 0, 0, 0), 
                         gluc_b2 = c(-1, 1, 0, 0, 0), 
                         gluc_b3 = c(-1, 0, 1, 0, 0), 
                         gluc_b4 = c(-1, 0, 0, 1, 0), 
                         gluc_b5 = c(-1, 0, 0, 0, 1))
# For the Bonferroni correction adjust "bonferroni" this is talbe 4.6
contrast(glucose_emmeans, Contrast_Table4.6, adjust="sidak")
contrast(glucose_emmeans, Contrast_Table4.6, adjust="bonferroni")

# Same contrasts with multicomp library

```

## Testing for Trend Across Categories
The coefficient estimates for the categories of physact from last section, decrease in order, suggesting that mean glucose levels are characterized by a linear trend across the levels of physact. Tests for linear trend are best performed using a contrast in the coefficients corresponding to the various levels of the categorical predictor.
Definition: A contrast is a weighted sum of the regression coefficients of the form $a_1\beta_1 + a_2\beta_2 + \cdots + a_p\beta_p$ in which the weights, or contrast coefficients, sum to zero: that is, $a_1 + a_2 + \cdots + a_p = 0$

```{r}
# 
Contrasts_glu <- list(MAvsLA          = c(-1, -1, 0,  1,  1),
                     MAvsLAforMuch   = c(-1,  0, 0,  0,  1),
                     MAvsLAforSome   = c( 0, -1, 0,  1,  0),
                     MLAvsC          = c(-1,  0, 1,  0,  0),
                     SLAvsC          = c( 0, -1, 1,  0,  0),
                     SMAvsC          = c( 0,  0,-1,  1,  0),
                     MMAvsC          = c( 0,  0,-1,  0,  1),
                     LinTrend_phys   = c(-2, -1, 0,  1,  2))

# compare the results with emmeans adjusted with Sidak, FWE.
contrast(glucose_emmeans, Contrasts_glu, adjust="bonferroni")
contrast(glucose_emmeans, Contrasts_glu, adjust="sidak")
# With adjust="none", results will be the same as the aov method.

# for other more general examples
# Using the multcomp library

ContrastGlucExa1 <- ("
Contrast.Name     MLA SLA AAA SMA MMA
 MAvsLA           -1  -1   0   1   1
 MAvsLAforMuch    -1   0   0   0   1
 MAvsLAforSome     0  -1   0   1   0
 MLAvsC           -1   0   1   0   0
 SLAvsC            0  -1   1   0   0
 SMAvsC            0   0  -1   1   0
 MMAvsC            0   0  -1   0   1
 LinearTrending   -2  -1   0   1   2
")

ContrastGlucExa1_Matriz = as.matrix(read.table(textConnection(ContrastGlucExa1), header=TRUE, row.names=1))
ContrastGlucExa1_Matriz
#
# glucose_fit_act came from lm(glucose ~ physact, data = hers_nodi)
Gluc_GenLinHypoth = glht(glucose_fit_act, linfct = mcp(physact = ContrastGlucExa1_Matriz))
#
Gluc_GenLinHypoth$linfct
summary(Gluc_GenLinHypoth, test=adjusted("single-step"))

```

# Confounding

The model $\tt lm(glucose \sim exercise, data = hersnodi)$, the unadjusted coefficient for exercise estimates the difference in mean glucose levels between two subgroups of the population of women with heart disease. But this comparison ignores other ways in which those subgroups may differ. In other words, the analysis does not take account of confounding of the association we see. Although the unadjusted coefficient may be useful for describing differences between subgroups, it would be risky to infer any causal connection between exercise and glucose on this basis. In contrast, the adjusted coefficient for exercise in the model 
$\tt lm(glucose \sim exercise + age + drinkany + BMI, data = hersnodi)$, takes account of the fact that women who exercise also have lower BMI and are slightly younger and more likely to report alcohol use, all factors which are associated with differences in glucose levels. While this adjusted model is clearly rudimentary, the underlying premise of multipredictor regression analysis of observational data is that with a sufficiently refined model, we can estimate causal effects, free or almost free of confounding. Confounding is when one predictor $x_i$ (independent variable) can depend in some of the same variables that the response function $Y$.

## Example: BMI and LDL

We turn to a relatively simple example, again using data from the HERS cohort. BMI and LDL cholesterol are both established heart disease risk factors. It is reasonable
to hypothesize that higher BMI leads to higher LDL in some causal sense. An unadjusted model for BMI and LDL is obtainde with $\tt lm(LDL \sim BMI, data = hers)$. The unadjusted estimate shows that average LDL increases .42mg/dL per unit increase in BMI. 

```{r}
# when one predictor (independent variable) can depend in some of the 
# same variables that the response function
# Example 4 
# Example of BMI (Body mass index) and LDL (colesterol) using the HERS cohort data Table 4.12

LDL_fit_bmi <- lm(LDL ~ BMI, data = hers)
S(LDL_fit_bmi)

```

However, age, ethnicity (nonwhite), smoking, and alcohol use (drinkany) may confound this unadjusted association. These covariates may either represent determinants of LDL or be proxies for such determinants, and are correlated with but almost surely not caused by BMI, and so may confound the BMI–LDL relationship.

```{r}

# example of LDL modeled with BMI and other factors
LDL_fit_all <- lm(LDL ~ BMI + age + nonwhite + smoking + drinkany, data = hers)
S(LDL_fit_all)

```
After adjustment for these four demographic and lifestyle factors, the estimated increase in average LDL is 0.36mg/dL per unit increase in BMI, an association that remains highly statistically significant. In addition, average LDL is estimated to be 5.2mg/dL higher among nonwhite women, after adjustment for between-group
differences in BMI, age, smoking, and alcohol use. The association of smoking with higher LDL is also statistically significant, and there is some evidence for lower LDL among older women and those who use alcohol.

# Interaction

```{r}
# Now for the group with Hormone Therapy the complete HERS cohort
# Model of cholesterol LDL and the effect of Hormone Therapy (HT) and
# Statin use model is at the one year visit (to se the HT effect)

hers <- mutate(hers, HT = factor(HT))
hers <- hers %>% mutate(HT = relevel(HT, ref = "placebo"))

# For the first year visit LDL levels at one year LDL1
LDL1_model <- lm(LDL1 ~ HT * statins, data = hers)
S(LDL1_model)

# For the table 4.15 of the book, to test for the linear 
# combination of the coefficients for the interaction to
# TEST the linear combination of the coefficients for HT
# and the statin interaction b1 and b3 the third part of table 4.15

coefeq <- matrix(data=0, nrow = 1, ncol = length(LDL1_model$coefficients))
colnames(coefeq) <- names(LDL1_model$coefficients)
coefeq
coefeq[1, "HThormone therapy"] <- 1
coefeq[1, "HThormone therapy:statinsyes"] <- 1
coefeq %*% LDL1_model$coefficients

# coeftest <- glht(model= LDL_model, linfct= coefeq, rhs= 0, alternative= "greater")
coeftest <- glht(model= LDL1_model, linfct= coefeq, rhs= 0)
summary(coeftest)

# Example of LDL after one year of HT and physact
# This is the table 4.16 of the book, effect of the hormone therapy
# combined with effects of physical activity at the one year visit
# now to estimate the linear combination of the coefficients
hers <- mutate(hers, physact = factor(physact, levels=c("much less active","somewhat less active","about as active","somewhat more active","much more active")))
LDL1phys_model <- lm(LDL1 ~ HT * physact, data = hers)
S(LDL1phys_model)

# The coefficients test for interaction
# can be done with glht()

coef_LDL1phys <- matrix(data=0, nrow = 1, ncol = length(LDL1phys_model$coefficients))
colnames(coef_LDL1phys) <- names(LDL1phys_model$coefficients)

# E[LDL|x] = b0 + b1 HT + b2 physact + b3 HT:physact
# we will look at slope = b3 BMIc; coeff = 0, 0, 0, 0, 0, 1, 1, 1, 1
# to test of significance

coef_LDL1phys[1, "HThormone therapy:physactsomewhat less active"] <- 1
coef_LDL1phys[1, "HThormone therapy:physactabout as active"] <- 1
coef_LDL1phys[1, "HThormone therapy:physactsomewhat more active"] <- 1
coef_LDL1phys[1, "HThormone therapy:physactmuch more active"] <- 1
coef_LDL1phys %*% LDL1phys_model$coefficients
coef_LDL1phys
coef_LDL1phystest <- glht(model= LDL1phys_model, linfct= coef_LDL1phys, rhs= 0)
S(coef_LDL1phystest)

# Example of BMI and statins, with interaction
# Model of LDL and BMI*statins and other variables
hers <- mutate(hers, statins = factor(statins))
hers <- mutate(hers, nonwhite = factor(nonwhite))
hers <- mutate(hers, smoking = factor(smoking))
hers <- mutate(hers, drinkany = factor(drinkany))
hers <- mutate(hers, BMIc = BMI - mean(BMI, na.rm=TRUE))
LDLbmi_model <- lm(LDL ~ statins*BMIc + age + nonwhite + smoking + drinkany, data = hers)
summary(LDLbmi_model)

# For the table 4.17 of the book, to test for the linear 
# combination of the coefficients for the interaction to
# TEST the linear combination of the coefficients for HT

coef_LDLbmi <- matrix(data=0, nrow = 1, ncol = length(LDLbmi_model$coefficients))
colnames(coef_LDLbmi) <- names(LDLbmi_model$coefficients)

# E[LDL|x] = b0 + b1 statins + b2 BMIc + b3 statins:BMIc + b4 age
#           + b5 nonwhite + b6 smoking + b7 + drinkany
# we will look at slope = b2 + b3 BMIc; coeff = 0, 0, 1, 1, 0, 0, 0, 0

coef_LDLbmi[1, "statinsyes"] <- 0
coef_LDLbmi[1, "BMIc"] <- 1
coef_LDLbmi[1, "statinsyes:BMIc"] <- 1
coef_LDLbmi %*% LDLbmi_model$coefficients
coef_LDLbmi
coef_LDLbmitest <- glht(model= LDLbmi_model, linfct= coef_LDLbmi, rhs= 0)
summary(coef_LDLbmitest)

# Comp of regular linear model and robust and some standard error handling
#
glucose_model <- lm(glucose ~ diabetes + BMI + age + drinkany, data= hers)
summary(glucose_model)

# MASS rlm()
glucose_rmodel <- rlm(glucose ~ diabetes + BMI + age + drinkany, data= hers)
summary(glucose_rmodel)
#
glucose_rmodel_bi <- rlm(glucose ~ diabetes + BMI + age + drinkany, psi= psi.bisquare, data= hers)
S(glucose_rmodel_bi)
#
```

